---
title: "Exercise Classifier"
author: "Matthew Thomas"
date: "`r Sys.Date()`"
output: html_document
---

```{r setup, include=FALSE, cache=TRUE}
knitr::opts_chunk$set(echo = TRUE)
```

## Synopsis

Using devices such as Jawbone Up, Nike FuelBand, and Fitbit, it is now possible 
to collect a large amount of data about personal activity relatively inexpensively. 
This type of devices is part of the quantified self movement â€“ a group of 
enthusiasts, who take measurements about themselves regularly to improve their health. 
One thing that people regularly do is quantify how much of a particular activity they do, 
but they rarely quantify how well they do it. In this project, our goal was 
to use data from accelerometers on the belt, forearm, arm, and dumbbell of six 
participants to assign a grade that rated how well they are exercising. 

## Exploratory Analysis

```{r, echo=FALSE}
suppressMessages({
        suppressWarnings(
        {
          library(dplyr)        # easier data frame manipulation
          library(ggplot2)      # advanced plotting library
          library(tidyverse)    # easier manipulation of data frames 
          library(caret)        # machine learning functionality
        })
})
```


```{r, echo=FALSE}
set.seed(0)

# Load data 
data <- read.csv('./assets/pml-training.csv')
post_test_data <- read.csv('./assets/pml-testing.csv')

# Remove columns with missing values
post_testing <- post_test_data %>% select_if(~ !any(is.na(.)))
# Remove columns with data not necessary for analysis
post_testing <- post_testing %>% select(!(X:num_window)) %>% select(!(problem_id))
# Normalize testing data 
post_testing <- as.data.frame(scale(post_testing))

# Divide data into training and testing data 
inTrain <- createDataPartition(y=data$classe, p=0.7,list=FALSE)
training <- data[inTrain, ]
testing <- data[-inTrain, ]

# Retrieve columns from post-test data 
col_names <- names(post_testing)
# Select same columns in training, testing data 
train <- training %>% select(all_of(col_names))
test <- testing %>% select(all_of(col_names))
# Normalize the data 
train <- as.data.frame(scale(train))
test <- as.data.frame(scale(test))
# Retrieve target values
train$classe <- as.factor(training$classe)
test$classe <- as.factor(testing$classe)
```
  
```{r, echo=FALSE}
# Look at columns 
str(train)
```

Overall, we have 13,737 observations in our training set with 52 features and the 
outcome `classe`, which is one of five grades: A, B, C, D, or E. We chose this combination 
of features because these are the features in the test set that aren't missing values. 
We also chose to normalize the features because many multi-class classifiers 
require normalized data. For ease of analysis, we also converted the outcome to 
a factor variable.  

Because we intended to use cross validation to determine the best classifier 
hyper-parameters and choose the best performing classifier, we divided the training
data using a 70-30 split into training and validation data sets. So to evaluate the 
performance of the classifiers, we used the validation data, not the actual test
data. 
           
```{r, echo=FALSE}
# Check amount of correlation between variables 
M <- abs(cor(train[, -53]))
diag(M) <- 0
correlated <- which(M > 0.8, arr.ind=T)
# Proportion of features correlated with other features
print('Proportion of features over 80% correlated:') 
(length(correlated) / 2) / length(train)
```

About 71.7% of features have correlation coefficients over 0.8. To account for this high
amount of correlation, we pre-processed the data using Principal Component Analysis (PCA) 
to reduce the number of features. 

## Classifier Models

Using accelerometer data on the belt, forearm, arm, and dumbell of the participants,
the goal of the project is to predict how well they did their exercises. To determine
how well they did the exercises, each exercise instance will be assigned one of 
five scores: A, B, C, D, or E.

While there are ways to do multi-class logistic regression, we chose not to 
try that approach because of the technique is overly complicated. Also, since there are
a lot of correlations between our features, we didn't attempt a Naive Bayes 
model because we can't assume the features are independent.

Of the many types of multi-class classifiers left, they fall into one of three 
families: nearest neighbors, decision trees, and support vector machines. We chose 
to implement one classifier of each type and compare their performances.

To determine the best hyper-parameters for each classifier, we chose to use 
cross validation. As a nice compromise between accuracy and computational 
complexity, we used 10-fold cross validation repeated three times. 

```{r, echo=FALSE}
# Cross Validation: 10 iterations, 3 folds 
train_control <- trainControl(method = "repeatedcv", 
                              number = 10, 
                              repeats = 3)
```

Because of the high number of highly correlated features, we used PCA to pre-process
the data before training each classifier. We then made predictions about the 
validation data and compared the results. As a final stage of the analysis, we created
two stacked models. For the first, we stacked the best performing pair of the 
classifiers. For the second one, we created a model consisting of all three of 
the classifiers, and compared the results of the stacked models to the single 
classifier models.


## Results:

### Baseline Classifiers:

```{r, echo=FALSE}
print('Proportions of each class in training data:')
values <- table(train$classe) / length(train$classe)
print(values)
```

#### Majority Class: 

One common type of baseline classifier is the Majority-Class classifier. The 
Majority-Class classifier always assigns the most common label in the training 
data to any new data. In this case, the class `A` is the most frequent, and 
assuming the new data has the same distribution of labels as the training data, 
we'd be correct about 28.4% of the time. 

#### Random Rate (Weighted Guessing):

Another common type of baseline classifier is the Random-Rate classifier. In the 
case of the Random-Rate classifier, we assign class labels based on the class 
distribution in the training data. In this case, we would randomly assign class A 
to 28.4% of the new observations, class B to 19.4% of the time, class C 17.4%, etc. 
The theoretical baseline accuracy using this strategy would be: 

```{r, echo=FALSE}
random_rate_acc <- values[1]^2 + values[2]^2 + 
                   values[3]^2 + values[4]^2 + values[5]^2
print('combined_accuracy = (proportion A)^2 + (proportion B)^2 + ...')
print(random_rate_acc)
```

So we would expect to be correct about 20.9% of the time. 

### k-Nearest Neighbor:

***k-Nearest Neighbor classifier with PCA Pre-processing:***
```{r, echo=FALSE}
# With PCA
kNN <- train(classe ~ ., 
               data = train, 
               method = "knn", 
               trControl = train_control,
               preProcess= 'pca')
print(kNN$results %>% select(k, Accuracy))
```

Overall, the most accurate k-Nearest Neighbor classifier used 5 nearest neighbors,
and had an accuracy of 95.2% on the training data. 

***Results of prediction with this classifier:***
```{r, echo=FALSE}
# Make predictions, print results
pred_kNN <- predict(kNN, test[, -53])
cm_1 <- confusionMatrix(pred_kNN, as.factor(testing$classe), 
                        mode='everything')
print(c(cm_1$overall['Accuracy'], cm_1$overall['AccuracyLower'], 
        cm_1$overall['AccuracyUpper'], cm_1$overall['AccuracyPValue']))
```

Using the classifier to make predictions on the validation data, our accuracy estimate
is 96.1%, and we're 95% sure that the real value is between 95.6% and 96.6%. 


### Support Vector Machine(SVM):
Because we assume that there are non-linear associations between the features, 
we chose to use the Radial Basis Function (RBF) kernel method for the SVM classifier, 
because it performs well with this type of data. 

***SVM classifier with PCA Pre-processing:***
```{r, echo=FALSE}
# Used to find best C, With PCA 
svm <- train(classe ~ ., 
               data = train, 
               method = "svmRadial", 
               trControl = train_control,
               preProcess= 'pca')

print(svm$results %>% select(sigma, C, Accuracy))
```

During cross validation, the best sigma value was estimated to be about 0.026, 
and holding this value constant, the best performing SVM classifier had a C value
of 1.0. On the training data, its accuracy was 90.4%. 

***Results of prediction with this classifier:***
```{r, echo=FALSE}
# Make predictions, print results
pred_svm <- predict(svm, test[, -53])
cm_2 <- confusionMatrix(pred_svm, as.factor(testing$classe))
print(c(cm_2$overall['Accuracy'], cm_2$overall['AccuracyLower'], 
        cm_2$overall['AccuracyUpper'], cm_2$overall['AccuracyPValue']))
```

Using these hyper-parameters, the SVM was about 90.5% accurate on the validation data. 
We're 95% sure that the actual accuracy is between 89.7% and 91.3%. 


### Decision Trees:
As the best performing type of Decision Tree, we chose to use the Random Forest 
classifier.

***Random Forest classifier with PCA Pre-processing:***
```{r, echo=FALSE}
# Used to find best mtry, with PCA 
suppressWarnings(
{ 
        rf <- train(classe ~ ., 
              data = train, 
              method = "rf", 
              trControl = train_control,
              preProcess= 'pca')
})
print(rf$results %>% select(mtry, Accuracy))
```

During cross validation, we determined the best `mtry` value, the number of 
features randomly sampled as candidates at each split, was 2. With this `mtry`
the Random Forest classifier was 97.2% accurate on the training data. 

***Results of prediction with this classifier:***
```{r, echo=FALSE}
# Make predictions, print results
pred_rf <- predict(rf, test[, -53]) 
cm_3 <- confusionMatrix(pred_rf, as.factor(testing$classe))
print(c(cm_3$overall['Accuracy'], cm_3$overall['AccuracyLower'], 
        cm_3$overall['AccuracyUpper'], cm_3$overall['AccuracyPValue']))
```

On the validation data, the Random Forest classifier had an accuracy estimate of 
97.0% with a confidence interval of 96.5% to 97.3%. 

```{r, echo=FALSE, fig.height=4, fig.width=6}

# Data frame with accuracy data 
bar_df1 <- data.frame(Classifier = c('k-Nearest Neighbor', 'Support Vector Machine',
                                      'Random Forest'),
                      Accuracy = c(cm_1$overall['Accuracy'], cm_2$overall['Accuracy'],
                                   cm_3$overall['Accuracy']))

# Bar chart comparing results 
p <- ggplot(bar_df1, aes(x = reorder(Classifier, Accuracy), y = Accuracy, fill=Classifier)) + 
        geom_bar(stat="identity") +
        scale_y_continuous(n.breaks=10) +
        labs(x="Type of Classifier", 
             y="Accuracy",
             title='Accuracy of Single Model Classifiers') +
        scale_fill_manual(values=c('deeppink3', 
                                   'darkblue',
                                   'darkorchid3')) +
        theme(legend.position='none')
print(p)

```

Comparing the single model classifiers, the k-Nearest Neighbor and 
the Random Forest classifiers were the best performing classifiers. 


### Stacked Models:

In addition to these single model classifiers, we created two stacked model classifiers.
For the first, we combined our two best performing classifiers, the k-Nearest 
Neighbor and the Random Forest, and for the second we combined all three classifiers. 
By combining the models, we hoped to improve overall performance. 

Since the k-Nearest Neighbor and SVM classifiers require the data to be normalized,
and the data in stacked model are the assigned labels from the other classifiers, 
we chose to use the Random Forest classifier for the stacked model. It was also 
our best performing classifier. 

***Stacked k-Nearest Neighbor and Random Forest classifier:***
```{r, echo=FALSE}
# kNN and Random forest
predDF_1 <- data.frame(pred_rf, pred_kNN, classe=testing$classe)
combModFit_1 <- train(classe ~ ., 
                      method='rf', 
                      data=predDF_1, 
                      trControl = train_control)

print(combModFit_1$results %>% select(mtry, Accuracy))
```

For the stacked model that combines our k-Nearest Neighbor and Random Forest
classifiers, the accuracy was 97.2% with an `mtry` of 5. 

***Results of prediction with this classifier:***
```{r, echo=FALSE}
# Make predictions, print results
combPred_1 <- predict(combModFit_1, predDF_1)
cm_7 <- confusionMatrix(combPred_1, as.factor(testing$classe))
print(c(cm_7$overall['Accuracy'], cm_7$overall['AccuracyLower'], 
        cm_7$overall['AccuracyUpper'], cm_7$overall['AccuracyPValue']))
```

The accuracy of the stacked model on the validation data was 97.3% with a 95% 
confidence interval of 96.96% to 97.7%. 


***Stacked k-Nearest Neighbor, Random Forest and SVM classifier:***
```{r, echo=FALSE}
# kNN, random forest, and SVM 
predDF_2 <- data.frame(pred_rf, pred_kNN, pred_svm, classe=testing$classe)
combModFit_2 <- train(classe ~ ., 
                      method='rf', 
                      data=predDF_2, 
                      trControl = train_control)

print(combModFit_2$results %>% select(mtry, Accuracy))
```

The accuracy on the training data for our stacked model combining all three 
classifiers was 97.3% with an `mtry` of 2. 


***Results of prediction with this classifier:***
```{r, echo=FALSE}
# Make predictions, print results
combPred_2 <- predict(combModFit_2, predDF_2)
cm_8 <- confusionMatrix(combPred_2, as.factor(testing$classe))
print(c(cm_8$overall['Accuracy'], cm_8$overall['AccuracyLower'], 
        cm_8$overall['AccuracyUpper'], cm_8$overall['AccuracyPValue']))
```

By combining all three classifiers, our accuracy estimate was 97.4%, and we're 
95% sure that the true accuracy is between 97.0% and 97.8%. 

```{r, echo=FALSE, fig.height=4, fig.width=6}

# Data frame with accuracy data 
bar_df2 <- data.frame(Classifier = c('k-Nearest Neighbor +\nRandom Forest', 
                                     'All Three Classifiers',
                                     'Random Forest'),
                      Accuracy = c(cm_7$overall['Accuracy'], cm_8$overall['Accuracy'],
                                   cm_3$overall['Accuracy']))

# Bar chart comparing results 
p <- ggplot(bar_df2, aes(x = reorder(Classifier, Accuracy), y = Accuracy, fill=Classifier)) + 
        geom_bar(stat="identity") +
        scale_y_continuous(n.breaks=10) +
        labs(x="Type of Classifier", 
             y="Accuracy",
             title='Accuracy of Best Performing Classifiers') +
        scale_fill_manual(values=c('darkgoldenrod3', 
                                   'darkolivegreen3', 
                                   'darkblue')) +
        theme(legend.position='none')
print(p)

```


## Conclusions

Overall, our best performing classifiers achieved accuracy rates above 97%. Compared
to a Random Rate baseline classifier, our classifiers added over 75% of value, 
and compared to a Majority Vote baseline classifier, we added almost 70% of value. 

As expected, the stacked models performed better than our best single model classifiers.
Also, adding more models to the stack improved performance. So for the test data,
we'll use the three classifier, stacked model, which had an accuracy of 97.4% 
on the validation data, and we'd expect a sample error between 97.0% and 97.8%, 
the confidence interval of this model's accuracy. 
