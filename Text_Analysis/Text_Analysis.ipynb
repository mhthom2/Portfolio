{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bb87084c-db92-4c74-8a46-63d26888541d",
   "metadata": {},
   "source": [
    "# Text Analysis and Spelling Recommender\n",
    "For this project, we used the `nltk` library to explore the <a href='http://www.cs.cmu.edu/~ark/personas/'>CMU Movie Summary Corpus</a>. All data is released under a Creative Commons Attribution-ShareAlike License. \n",
    "\n",
    "We also created a spelling recommender function that uses the `nltk` library to find words similar to the misspelling. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "9033778e-0ab0-4332-9dfc-b30c340c83f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.probability import FreqDist\n",
    "from nltk.corpus import words\n",
    "from nltk.metrics.distance import jaccard_distance\n",
    "from nltk.util import ngrams\n",
    "from nltk.metrics.distance import edit_distance\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "# nltk downloads\n",
    "# nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4b0e876-c233-4c22-bd1b-b5f1e03006b2",
   "metadata": {},
   "source": [
    "## 1. Exploratory Analysis of Plot Summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fced317a-3c5a-4c22-b933-3dd35b0d7e02",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Plot</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Shlykov, a hard-working taxi driver and Lyosha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The nation of Panem consists of a wealthy Capi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Poovalli Induchoodan  is sentenced for six yea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The Lemon Drop Kid , a New York City swindler,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Seventh-day Adventist Church pastor Michael Ch...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Plot\n",
       "0  Shlykov, a hard-working taxi driver and Lyosha...\n",
       "1  The nation of Panem consists of a wealthy Capi...\n",
       "2  Poovalli Induchoodan  is sentenced for six yea...\n",
       "3  The Lemon Drop Kid , a New York City swindler,...\n",
       "4  Seventh-day Adventist Church pastor Michael Ch..."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load / prep raw data\n",
    "df = pd.read_csv('./assets/plot_summaries.txt', sep='\\t', header=None)\n",
    "df = df[[1]]\n",
    "df = df.rename(columns={1 : 'Plot'})\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cec984eb-b82c-4ad0-bedc-a6c718c58a3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine plots into single string\n",
    "plot_string = df.sum()\n",
    "# Tokenize that string\n",
    "tokens = nltk.word_tokenize(plot_string['Plot'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8d8aa85-3a8c-46c1-8d97-3954dd689d1a",
   "metadata": {},
   "source": [
    "### How many tokens, including both words and punctuation marks, are in the text?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "44fab80a-e49b-4110-9fa5-7a741574ffde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are   14,837,137  words and punctuation marks in the text.\n"
     ]
    }
   ],
   "source": [
    "# Find number of tokens\n",
    "num_tokens = len(tokens)\n",
    "\n",
    "# Format number of tokens\n",
    "str_num_tokens = f'{num_tokens : ,}'\n",
    "print('There are ', str_num_tokens, ' words and punctuation marks in the text.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37a57ac8-3fb2-4ea8-b448-d1d658ec5149",
   "metadata": {},
   "source": [
    "### How many unique tokens are there in text?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "691bf077-a30e-488d-b71d-10dc49e239e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are   226,452  unique words and punctuation marks in the text.\n"
     ]
    }
   ],
   "source": [
    "# Find number of unique tokens\n",
    "num_unique = len(set(tokens))\n",
    "                 \n",
    "# Format number of unique tokens\n",
    "str_num_unique = f'{num_unique : ,}'\n",
    "print('There are ', str_num_unique, ' unique words and punctuation marks in the text.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3df51613-286f-42a0-945c-f61bda7cbe83",
   "metadata": {},
   "source": [
    "### After lemmatizing the verbs, how many unique tokens does the text have?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "24491341-10d4-4178-985b-e32eceb2f998",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are   213,430  unique lemmatized verbs in the text.\n"
     ]
    }
   ],
   "source": [
    "# Create lemmatizer \n",
    "lemmatizer = WordNetLemmatizer()\n",
    "# Lemmatize verbs \n",
    "lemmatized = [lemmatizer.lemmatize(w,'v') for w in tokens]\n",
    "\n",
    "# Find number of unique lemmatized verbs\n",
    "num_lemmatized = len(set(lemmatized))\n",
    "# Format number of unique lemmatized verbs\n",
    "str_num_lemmatized = f'{num_lemmatized : ,}'\n",
    "\n",
    "print('There are ', str_num_lemmatized, ' unique lemmatized verbs in the text.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9b0a4c7-eba8-471f-8834-dd03f4bf55dd",
   "metadata": {},
   "source": [
    "### What is the lexical diversity of the given text input? \n",
    "The lexical diversity of a text is the ratio of unique tokens to the total number of tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "801bb078-8cb3-4b58-bc7d-d1d93e05df41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The lexical diversity is:  0.015\n"
     ]
    }
   ],
   "source": [
    "# Calculate lexical diversity\n",
    "lex_diversity = round(num_unique/num_tokens, 3)\n",
    "\n",
    "print('The lexical diversity is: ', str(lex_diversity))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15fb0a81-0ea9-4be5-9c4f-91412e4226dd",
   "metadata": {},
   "source": [
    "This level of diversity is rare for texts because it indicates that the content is repetitive or simplistic, like childrenâ€™s rhymes or chants. However, lexical diversity is sensitive to text length. So large texts, like this one, naturally have lower diversity. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74b7ba71-0663-4518-8e45-2e6155a17c31",
   "metadata": {},
   "source": [
    "### What percentage of tokens is 'love'or 'Love'?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "09bb6c69-6a1a-42c5-835a-f9d7d9356755",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The percentage of tokens that are 'love' or 'Love' is   0.12 %.\n"
     ]
    }
   ],
   "source": [
    "# Frequency distribution of tokens\n",
    "fdist = FreqDist(tokens)\n",
    "# Add frequencies for 'love' and 'Love'\n",
    "freq_love = fdist.get('love') + fdist.get('Love')\n",
    "\n",
    "# Find the percentage\n",
    "perc_love = round((freq_love / num_tokens) * 100, 2)\n",
    "# Convert to string \n",
    "str_perc_love = f'{perc_love : ,}'\n",
    "\n",
    "print(\"The percentage of tokens that are 'love' or 'Love' is \", str_perc_love, '%.') "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "888fe721-8a22-4f59-a91d-639db1ca2733",
   "metadata": {},
   "source": [
    "### Which 10 unique tokens appear most frequently in the text? What is their frequencies?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "25ba15ef-22d3-42d1-9dee-c712a5fb3ad2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Token</th>\n",
       "      <th>Frequency</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>,</td>\n",
       "      <td>787499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>the</td>\n",
       "      <td>737267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>.</td>\n",
       "      <td>619554</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>to</td>\n",
       "      <td>478162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>and</td>\n",
       "      <td>455448</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>a</td>\n",
       "      <td>362327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>of</td>\n",
       "      <td>261133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>is</td>\n",
       "      <td>225160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>in</td>\n",
       "      <td>201022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>his</td>\n",
       "      <td>190723</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Token  Frequency\n",
       "0     ,     787499\n",
       "1   the     737267\n",
       "2     .     619554\n",
       "3    to     478162\n",
       "4   and     455448\n",
       "5     a     362327\n",
       "6    of     261133\n",
       "7    is     225160\n",
       "8    in     201022\n",
       "9   his     190723"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get 10 most frequent tokens\n",
    "most_freq = fdist.most_common(10)\n",
    "\n",
    "# Create data frame with results\n",
    "top_10 = pd.DataFrame(most_freq, columns=['Token', 'Frequency'])\n",
    "\n",
    "top_10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "341e5fb8-1b1c-4e66-8cf2-e13655d770b7",
   "metadata": {},
   "source": [
    "### What tokens have a length of greater than 5 and frequency of more than 10,000?\n",
    "The tokens should be sorted alphabetically. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "aeaf2cb2-f141-4db3-a249-f1221ab0c809",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Frequent Long Tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>However</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>becomes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>before</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>begins</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>family</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>father</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>friend</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>himself</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>killed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>mother</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>police</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>through</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Frequent Long Tokens\n",
       "0               However\n",
       "1               becomes\n",
       "2                before\n",
       "3                begins\n",
       "4                family\n",
       "5                father\n",
       "6                friend\n",
       "7               himself\n",
       "8                killed\n",
       "9                mother\n",
       "10               police\n",
       "11              through"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get tokens \n",
    "unique_tokens = fdist.keys()\n",
    "\n",
    "# Find tokens with len > 5 and frequency > 10,000 \n",
    "long_freq_tokens = [t for t in unique_tokens if len(t) > 5 and fdist[t] > 10000]\n",
    "\n",
    "# Sort list \n",
    "sorted_long_freq_tokens = sorted(freq_tokens)\n",
    "# Create data frame\n",
    "sorted_long_freq_df = pd.DataFrame(sorted_long_freq_tokens, columns=['Frequent Long Tokens'])\n",
    "\n",
    "sorted_long_freq_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9267beff-f3d5-4428-9454-5730166d4e09",
   "metadata": {},
   "source": [
    "### Find the longest token. What's its length?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "8b850425-ab0c-47c9-a6bb-12282ab2204d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The longest token is \" //www.rottentomatoes.com/m/new_brooklyn/articles/1801319/exquisitely_constructed_it_gives_viewers_a_consistently_satisfying_experience_which_works_on_every_level_but_lewin_steals_the_movie_be_ready_to_have_your_heart_break \" and it is  222 characters long.\n"
     ]
    }
   ],
   "source": [
    "# Sort in reverse order by length \n",
    "sorted_tokens = sorted(unique_tokens, key=len, reverse=True)\n",
    "\n",
    "# Find length of the longest token\n",
    "len_longest = len(sorted_tokens[0])\n",
    "\n",
    "print('The longest token is \"', sorted_tokens[0], '\" and it is ', len_longest, 'characters long.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84b45b62-ba8c-4b5f-8c04-b0d26cc7674a",
   "metadata": {},
   "source": [
    "### What unique *words* have a frequency of more than 100,000? What is their frequency?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "64b6659f-05d0-41c9-add6-b89e67575718",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Frequency</th>\n",
       "      <th>Word</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>737267</td>\n",
       "      <td>the</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>478162</td>\n",
       "      <td>to</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>455448</td>\n",
       "      <td>and</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>362327</td>\n",
       "      <td>a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>261133</td>\n",
       "      <td>of</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>225160</td>\n",
       "      <td>is</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>201022</td>\n",
       "      <td>in</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>190723</td>\n",
       "      <td>his</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>147668</td>\n",
       "      <td>her</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>139296</td>\n",
       "      <td>he</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>137142</td>\n",
       "      <td>that</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>134820</td>\n",
       "      <td>with</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Frequency  Word\n",
       "0      737267   the\n",
       "1      478162    to\n",
       "2      455448   and\n",
       "3      362327     a\n",
       "4      261133    of\n",
       "5      225160    is\n",
       "6      201022    in\n",
       "7      190723   his\n",
       "8      147668   her\n",
       "9      139296    he\n",
       "10     137142  that\n",
       "11     134820  with"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find words with frequency > 100,000 \n",
    "freq_words = [(fdist[t], t) for t in unique_tokens if t.isalpha() and fdist[t] > 100000]\n",
    "# Sort this list of tuples \n",
    "sorted_freq_words = sorted(freq_words, key=lambda t: t[0], reverse=True)\n",
    "\n",
    "# Create data frame with most frequent words in descending order of frequency \n",
    "sorted_freq_words_df = pd.DataFrame(sorted_freq_words, columns=['Frequency', 'Word'])\n",
    "sorted_freq_words_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5a50b5f-a4ad-48c2-a671-d9a246990062",
   "metadata": {},
   "source": [
    "### What are the 5 most frequent parts of speech in the text? What is their frequency?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "04f9cac4-daf7-41b5-b1b5-ef3a88fab5ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Part of Speech</th>\n",
       "      <th>Frequency</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NN</td>\n",
       "      <td>2046873</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>IN</td>\n",
       "      <td>1551928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NNP</td>\n",
       "      <td>1514305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>DT</td>\n",
       "      <td>1358231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>VBZ</td>\n",
       "      <td>926175</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Part of Speech  Frequency\n",
       "0             NN    2046873\n",
       "1             IN    1551928\n",
       "2            NNP    1514305\n",
       "3             DT    1358231\n",
       "4            VBZ     926175"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Identify part of speech\n",
    "pos = nltk.pos_tag(tokens)\n",
    "# Count parts of speech \n",
    "pos_count = Counter(tup[1] for tup in pos)\n",
    "\n",
    "# Five most frequent \n",
    "pos_top_5 = pos_count.most_common(5)\n",
    "# Create data frame with five most frequent pos\n",
    "pos_top_5_df = pd.DataFrame(pos_top_5, columns=['Part of Speech', 'Frequency'])\n",
    "\n",
    "pos_top_5_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffbc8faf-c0af-4439-a31f-94cddf4e42a5",
   "metadata": {},
   "source": [
    "So the most common parts of speech are singular nouns (NN), prepositions or subordinating conjunctions (IN), singular proper nouns (NNP), determiners or articles (DT), and third person singular verbs (VBZ)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfec15ab-5eba-4f67-93cc-31b65ed86edc",
   "metadata": {},
   "source": [
    "## 2. Spelling Recommender\n",
    "For this part project, we created three different spelling recommenders. Each takes a list of misspelled words and recommends a correctly spelled word for every word in the list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "78a0e810-8f3f-4687-974d-f05c832e92d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A comprehensive corpus of words \n",
    "correct_spellings = words.words()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67811a7a-c0ad-492b-bf29-1974411469d8",
   "metadata": {},
   "source": [
    "### Recommender Using Jaccard Similarity Index on Trigrams\n",
    "The first recommender uses the Jaccard Similarity index and character trigrams to recommend a correct spelling. For each of the misspelled words, we find the character trigrams for each of them, and find the trigrams for each of the words in the corpus. Then we calculate the Jaccard Similarity index between the misspelled-word trigram set and all of the character trigram sets for the corpus words. The recommender suggests that the corpus word with the highest index is the correct spelling.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "ab4ff5d1-3b28-41a8-b4f2-62f594bd15f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommender_one(entries):\n",
    "\n",
    "    # List of recommendations\n",
    "    recommendations = []\n",
    "    # Size of n-grams \n",
    "    n = 3\n",
    "    \n",
    "    # Loop through arguments \n",
    "    for entry in entries:\n",
    "        # List of tuples with correct_spelling words and Jaccard distance\n",
    "        jaccard_dists = []\n",
    "        # n-gram of the argument\n",
    "        entry_n_gram = set(ngrams(entry, n))\n",
    "        # First letter of argument \n",
    "        entry_start_letter = entry[0]\n",
    "        \n",
    "        # Loop through words in correct_spellings \n",
    "        for word in correct_spellings:\n",
    "            \n",
    "            # To save time, check if starts with same letter \n",
    "            if word.startswith(entry_start_letter):\n",
    "                # Create n-gram of word in correct_spelling\n",
    "                word_n_gram = set(ngrams(word, n))\n",
    "                # Calculate Jaccard distance with argument \n",
    "                jaccard_dist = jaccard_distance(entry_n_gram, word_n_gram)\n",
    "                # Add tuple with word and its distance score\n",
    "                jaccard_dists.append((word, jaccard_dist))\n",
    "        \n",
    "        # Sort by Jaccard distances \n",
    "        sorted_jaccard_dists = sorted(jaccard_dists, key=lambda t: t[1])\n",
    "        # Top recommendation is the first word \n",
    "        recommendations.append(sorted_jaccard_dists[0][0]) \n",
    "    \n",
    "    # Return recommendations \n",
    "    return recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "0b96f87c-1b14-46f2-9199-321291a42d66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For the list:  ['cormulent', 'incendenece', 'validrate']\n",
      "these are recommended spellings:  ['corpulent', 'indecence', 'validate']\n"
     ]
    }
   ],
   "source": [
    "# List of misspelled words \n",
    "misspelled_words = ['cormulent', 'incendenece', 'validrate'] \n",
    "# Find recommendations\n",
    "recommendations_1 = recommender_one(misspelled_words)\n",
    "\n",
    "print('For the list: ', misspelled_words) \n",
    "print('these are recommended spellings: ', recommendations_1) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8cdb668-b1a1-4bea-b903-67a95bb06465",
   "metadata": {},
   "source": [
    "### Recommender Using Jaccard Distance on Quadrigrams\n",
    "For the second recommender, we use same process as the first recommender with one exception. Instead finding the trigrams of the words, we find the character quadrigrams. The recommender again suggests that the corpus-word quadrigram set that's most similar to that of the misspelled words is correct spelling. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "22476aaa-2dc4-4b37-a277-c6d7ff30a121",
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommender_two(entries):\n",
    "    \n",
    "    # List of recommendations\n",
    "    recommendations = []\n",
    "    # Size of n-grams \n",
    "    n = 4\n",
    "    \n",
    "    # Loop through arguments \n",
    "    for entry in entries:\n",
    "        # List of tuples with correct_spelling words and Jaccard distance\n",
    "        jaccard_dists = []\n",
    "        # n-gram of the argument\n",
    "        entry_n_gram = set(ngrams(entry, n))\n",
    "        # First letter of argument \n",
    "        entry_start_letter = entry[0]\n",
    "        \n",
    "        # Loop through words in correct_spellings \n",
    "        for word in correct_spellings:\n",
    "            # To save time, check if starts with same letter \n",
    "            if word.startswith(entry_start_letter):\n",
    "                # Create n-gram of word in correct_spelling\n",
    "                word_n_gram = set(ngrams(word, n))\n",
    "                # Calculate Jaccard distance with argument \n",
    "                jaccard_dist = jaccard_distance(entry_n_gram, word_n_gram)\n",
    "                # Add tuple with word and its distance score\n",
    "                jaccard_dists.append((word, jaccard_dist))\n",
    "                \n",
    "        # Sort by Jaccard distances \n",
    "        sorted_jaccard_dists = sorted(jaccard_dists, key=lambda t: t[1])\n",
    "        # Top recommendation is the first word \n",
    "        recommendations.append(sorted_jaccard_dists[0][0]) \n",
    "        \n",
    "    # Return recommendations \n",
    "    return recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "ca886d59-9467-441f-8499-322114d8775e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For the list:  ['cormulent', 'incendenece', 'validrate']\n",
      "these are recommended spellings:  ['cormus', 'incendiary', 'valid']\n"
     ]
    }
   ],
   "source": [
    "# Find recommendations for same misspelled words\n",
    "recommendations_2 = recommender_two(misspelled_words)\n",
    "\n",
    "print('For the list: ', misspelled_words) \n",
    "print('these are recommended spellings: ', recommendations_2) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "201ab13c-c82b-48df-af2d-cca31e248552",
   "metadata": {},
   "source": [
    "### Recommender Using Edit Distance \n",
    "For each misspelled word, we calculate edit distance with tranposition between those words and the corpus words. We suggest the corpus word with the shortest edit distance is the correct spelling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "b5f92b62-d1cf-421c-94c0-7aa164318625",
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommender_three(entries):\n",
    "\n",
    "    # List of recommendations\n",
    "    recommendations = []\n",
    "    \n",
    "    # Loop through arguments \n",
    "    for entry in entries:\n",
    "        # List of words in correct_spellings and edit distances \n",
    "        edit_dists = []\n",
    "        # First letter of argument \n",
    "        entry_start_letter = entry[0]\n",
    "\n",
    "        # Loop through words in correct_spellings \n",
    "        for word in correct_spellings:\n",
    "            # To save time, check if starts with same letter \n",
    "            if word.startswith(entry_start_letter):\n",
    "                # Calculate the edit distance\n",
    "                edit_dist = edit_distance(entry, word, transpositions=True)\n",
    "                # Add word and its distance as a tuple \n",
    "                edit_dists.append((word, edit_dist))\n",
    "                \n",
    "        # Sort by edit distances \n",
    "        sorted_edit_dists = sorted(edit_dists, key=lambda t: t[1])\n",
    "        # Recommendation is the first word \n",
    "        recommendations.append(sorted_edit_dists[0][0])\n",
    "        \n",
    "    # Return recommendations \n",
    "    return recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "9d8e3b73-440c-4b9a-8368-d995c2b1690f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For the list:  ['cormulent', 'incendenece', 'validrate']\n",
      "these are recommended spellings:  ['corpulent', 'intendence', 'validate']\n"
     ]
    }
   ],
   "source": [
    "# Find recommendations for same misspelled words\n",
    "recommendations_3 = recommender_three(misspelled_words)\n",
    "\n",
    "print('For the list: ', misspelled_words) \n",
    "print('these are recommended spellings: ', recommendations_3) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da0cefea-f0b4-445f-983f-c4e8f3362421",
   "metadata": {},
   "source": [
    "For the misspelled words `cormulent`, `incendenece`, and `validrate`, the recommenders using the Jaccard Similarity index on character trigrams and the edit distance suggested the same correct spellings, which seem to the most logical recommendations: `corpulent`, `intendence`, `validate`. \n",
    "\n",
    "The recommender using the Jaccard Similarity index on character quadrigrams suggested these words: `cormus`, `incendiary`, `valid`. While similar to the misspelled words, they don't seem to correct words. I think this is due to overfitting. In general, as the length of the character n-grams increase, the recommender performance will improve up to a point at which the suggestions will be less useful. It seems that the tipping point for this corpus is for n-grams of length 4.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13def9d1-cfd8-4d30-a43f-04feb24c0a73",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
